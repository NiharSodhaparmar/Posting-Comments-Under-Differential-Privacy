{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY3j38o5sg0V"
      },
      "outputs": [],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from operator import itemgetter\n",
        "from tqdm import tqdm\n",
        "import csv"
      ],
      "metadata": {
        "id": "sG7UovhHszT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "def load_data(data_file):\n",
        "    df = pd.read_csv(data_file)\n",
        "    df.fillna(\"\", inplace=True)\n",
        "    tweets = df['Tweet'].tolist()\n",
        "    labels = [0 if label == \"Democrat\" else 1 for label in df['Party'].tolist()]\n",
        "    return np.array(tweets), np.array(labels)\n",
        "\n",
        "tweets, labels = load_data('/content/drive/MyDrive/Datasets/Tweets/ExtractedTweets.csv')"
      ],
      "metadata": {
        "id": "n7g-HPa6s2aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad sequences\n",
        "max_words = 50000\n",
        "max_len = 128\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(tweets)\n",
        "sequences = tokenizer.texts_to_sequences(tweets)\n",
        "tweets = pad_sequences(sequences, maxlen=max_len)"
      ],
      "metadata": {
        "id": "P1O-6Uwys5nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "BraICfr0s8at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN model\n",
        "embedding_dim = 128"
      ],
      "metadata": {
        "id": "a-xLZqRms--o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
        "    SpatialDropout1D(0.2),\n",
        "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "7qas92eqtDJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "6OekGB0JtVIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the RNN model\n",
        "history = rnn_model.fit(X_train, y_train, epochs=3, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "XDDSqeSXtXLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred = (rnn_model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate accuracy and F1 score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f'F1 Score: {f1:.4f}')"
      ],
      "metadata": {
        "id": "BfKDW4WQtY8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prediction function for LIME\n",
        "def predict_proba(texts):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "    return np.array([[1 - pred[0], pred[0]] for pred in rnn_model.predict(padded_sequences)])"
      ],
      "metadata": {
        "id": "SQ92UW5itdCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a LIME explainer\n",
        "explainer = LimeTextExplainer(class_names=['Democrat', 'Republican'])"
      ],
      "metadata": {
        "id": "8lQg0lLitg2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to explain predictions\n",
        "def explain_prediction(tweet):\n",
        "    explanation = explainer.explain_instance(tweet, predict_proba, num_features=10)\n",
        "    return explanation"
      ],
      "metadata": {
        "id": "uLFwMFRXtiln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_tuples_array_by_second_item(tuples):\n",
        "    return sorted(tuples, key=itemgetter(1))"
      ],
      "metadata": {
        "id": "8Z7qtLo7tk-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract strong words using LIME\n",
        "words = {}\n",
        "wordsForCSV = []"
      ],
      "metadata": {
        "id": "90bcmnNDtodg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_explained_words(txt):\n",
        "    prediction = 'Democrat' if predict_proba([txt])[0][1] < 0.5 else 'Republican'\n",
        "    exp = explain_prediction(txt)\n",
        "\n",
        "    exp_list = [(x[1][0], x[1][1], x[0][0]) for x in zip(exp.local_exp[1], exp.as_list())]\n",
        "    democrat_list = sort_tuples_array_by_second_item([x for x in exp_list if x[1] < 0])\n",
        "    republican_list = sort_tuples_array_by_second_item([x for x in exp_list if x[1] > 0])\n",
        "\n",
        "    if prediction == \"Democrat\":\n",
        "        for mc in democrat_list[:2]:  # get top 2\n",
        "            if (mc[0], 0) in words:\n",
        "                words[(mc[0], 0)]['lime_score'].append(mc[1])\n",
        "            else:\n",
        "                words[(mc[0], 0)] = {'lime_score': [mc[1]], 'position': mc[2]}\n",
        "                wordsForCSV.append([mc[0], 0, mc[1]])\n",
        "    else:\n",
        "        for mc in republican_list[-2:]:  # get top 2\n",
        "            if (mc[0], 1) in words:\n",
        "                words[(mc[0], 1)]['lime_score'].append(mc[1])\n",
        "            else:\n",
        "                words[(mc[0], 1)] = {'lime_score': [mc[1]], 'position': mc[2]}\n",
        "                wordsForCSV.append([mc[0], 1, mc[1]])\n",
        "\n",
        "    return words, wordsForCSV\n"
      ],
      "metadata": {
        "id": "iwKq3wbjtscN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in tqdm(tweets, total=len(tweets)):\n",
        "    words, wordsForCSV = get_max_explained_words(tweet)"
      ],
      "metadata": {
        "id": "lV6MEp0kt5Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save extracted words to CSV\n",
        "header = [\"word\", \"label\", \"lime_score\"]\n",
        "with open('/content/drive/MyDrive/Datasets/Tweets/extracted_strong_words_rnn.csv', 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(wordsForCSV)"
      ],
      "metadata": {
        "id": "kLxmivMPt8z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUrlSEg5t_py"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}