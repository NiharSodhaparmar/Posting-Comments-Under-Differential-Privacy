{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a95Ql5DJ5mMc",
    "outputId": "3d892439-d1fa-46c9-98af-2b56ace42cca"
   },
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjWEx4VisPns"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJ3nXsvHsVDG",
    "outputId": "3d95a7b9-7ff4-480d-e925-6929869e2ee3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Nsu6nzBsXd7"
   },
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "  # read csv file\n",
    "  df = pd.read_csv(data_file)\n",
    "\n",
    "  # replace nan(no value) comment with \"\"(empty string)\n",
    "  df.fillna(\"\", inplace=True)\n",
    "\n",
    "  tweets = df['Tweet'].tolist()\n",
    "  labels = df['Party'].tolist()\n",
    "\n",
    "  labels = [0 if label == \"Democrat\" else 1 for label in labels]\n",
    "\n",
    "  return tweets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjyZm4GesY_b"
   },
   "outputs": [],
   "source": [
    "tweets, labels = load_data('/content/drive/MyDrive/Datasets/Tweets/ExtractedTweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eQFlCNodYod",
    "outputId": "081a8755-3a06-4485-e045-00bedcfd4799"
   },
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibfTdMG-cG2B"
   },
   "outputs": [],
   "source": [
    "new_tweets = tweets[73098:86460]\n",
    "new_labels = labels[73098:86460]\n",
    "\n",
    "n = int(len(new_tweets) / 17)\n",
    "x = [tweets[i:i + n] for i in range(0, len(new_tweets), n)]\n",
    "y = [labels[i:i + n] for i in range(0, len(new_labels), n)]\n",
    "\n",
    "first_half_tweets = x[0]\n",
    "second_half_tweets = x[1]\n",
    "third_half_tweets = x[2]\n",
    "fourth_half_tweets = x[3]\n",
    "fifth_half_tweets = x[4]\n",
    "sixth_half_tweets = x[5]\n",
    "seventh_half_tweets = x[6]\n",
    "eighth_half_tweets = x[7]\n",
    "ninth_half_tweets = x[8]\n",
    "tenth_half_tweets = x[9]\n",
    "eleventh_half_tweets = x[10]\n",
    "twelveth_half_tweets = x[11]\n",
    "thirteenth_half_tweets = x[12]\n",
    "fourteenth_half_tweets = x[13]\n",
    "fifteen_half_tweets = x[14]\n",
    "sixteen_half_tweets = x[15]\n",
    "seventeen_half_tweets = x[16]\n",
    "# eighteen_half_tweets = x[17]\n",
    "# nineteen_half_tweets = x[18]\n",
    "# twenty_half_tweets = x[19]\n",
    "# twentyone_half_tweets = x[20]\n",
    "# twentytwo_half_tweets = x[21]\n",
    "# twentythree_half_tweets = x[22]\n",
    "# twentyfour_half_tweets = x[23]\n",
    "# twentyfive_half_tweets = x[24]\n",
    "# twentysix_half_tweets = x[25]\n",
    "# twentyseven_half_tweets = x[26]\n",
    "# twentyeight_half_tweets = x[27]\n",
    "# twentynine_half_tweets = x[28]\n",
    "# thirty_half_tweets = x[29]\n",
    "# thirtyone_half_tweets = x[30]\n",
    "# thirtytwo_half_tweets = x[31]\n",
    "# thirtythree_half_tweets = x[32]\n",
    "# thirtyfour_half_tweets = x[33]\n",
    "# thirtyfive_half_tweets = x[34]\n",
    "# thirtysix_half_tweets = x[35]\n",
    "# thirtyseven_half_tweets = x[36]\n",
    "# thirtyeight_half_tweets = x[37]\n",
    "# thirtynine_half_tweets = x[38]\n",
    "# fourty_half_tweets = x[39]\n",
    "# fourtyone_half_tweets = x[40]\n",
    "# fourtytwo_half_tweets = x[41]\n",
    "# fourtythree_half_tweets = x[42]\n",
    "# fourtyfour_half_tweets = x[43]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "first_half_labels = y[0]\n",
    "second_half_labels = y[1]\n",
    "third_half_labels = y[2]\n",
    "fourth_half_labels = y[3]\n",
    "fifth_half_labels = x[4]\n",
    "sixth_half_labels = x[5]\n",
    "seventh_half_labels = x[6]\n",
    "eighth_half_labels = x[7]\n",
    "ninth_half_labels = x[8]\n",
    "tenth_half_labels = x[9]\n",
    "eleventh_half_labels = x[10]\n",
    "twelveth_half_labels = x[11]\n",
    "thirteenth_half_labels = x[12]\n",
    "fourteenth_half_labels = x[13]\n",
    "fifteen_half_labels = x[14]\n",
    "sixteen_half_labels = x[15]\n",
    "seventeen_half_labels = x[16]\n",
    "# eighteen_half_labels = x[17]\n",
    "# nineteen_half_labels = x[18]\n",
    "# twenty_half_labels = x[19]\n",
    "# twentyone_half_labels = x[20]\n",
    "# twentytwo_half_labels = x[21]\n",
    "# twentythree_half_labels = x[22]\n",
    "# twentyfour_half_labels = x[23]\n",
    "# twentyfive_half_labels = x[24]\n",
    "# twentysix_half_labels = x[25]\n",
    "# twentyseven_half_labels = x[26]\n",
    "# twentyeight_half_labels = x[27]\n",
    "# twentynine_half_labels = x[28]\n",
    "# thirty_half_labels = x[29]\n",
    "# thirtyone_half_labels = x[30]\n",
    "# thirtytwo_half_labels = x[31]\n",
    "# thirtythree_half_labels = x[32]\n",
    "# thirtyfour_half_labels = x[33]\n",
    "# thirtyfive_half_labels = x[34]\n",
    "# thirtysix_half_labels = x[35]\n",
    "# thirtyseven_half_labels = x[36]\n",
    "# thirtyeight_half_labels = x[37]\n",
    "# thirtynine_half_labels = x[38]\n",
    "# fourty_half_labels = x[39]\n",
    "# fourtyone_half_labels = x[40]\n",
    "# fourtytwo_half_labels = x[41]\n",
    "# fourtythree_half_labels = x[42]\n",
    "# fourtyfour_half_labels = x[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-Odt9AusoPW"
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, tokenizer, max_length):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = str(self.tweets[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'tweet_text': tweet,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bo4IFHvYsstc"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(tweets, labels, tokenizer, max_length, batch_size):\n",
    "    dataset = TweetDataset(tweets, labels, tokenizer, max_length)\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iz3-cS-KtC7F"
   },
   "outputs": [],
   "source": [
    "output_dir = '/content/drive/MyDrive/Datasets/Tweets/Saved Model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLbiJ9i-s7DU",
    "outputId": "9d7790c8-6017-4141-ffb3-983f49496f87"
   },
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print('Model and tokenizer loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVlAPZRbvVms"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    tweets = []\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            tweets.extend(d[\"tweet_text\"])\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return tweets, predictions, true_labels\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    return accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAqyGRnO5X56"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "MAX_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gr3dfV0fvnUs",
    "outputId": "2417f13f-4e4c-4f2f-cd22-98edfdb6a677"
   },
   "outputs": [],
   "source": [
    "val_data_loader = create_data_loader(tweets, labels, tokenizer, MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dqeuka2P2Ob6"
   },
   "outputs": [],
   "source": [
    "# # Evaluate model\n",
    "# tweets, predictions, true_labels = evaluate_model(model, val_data_loader, device)\n",
    "\n",
    "# # Calculate metrics\n",
    "# accuracy, f1 = calculate_metrics(true_labels, predictions)\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-imMHWHu_IaS"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Use autocast and GradScaler for mixed precision\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBHtILDD2Wee"
   },
   "outputs": [],
   "source": [
    "# Define the prediction function\n",
    "def predict_proba(texts):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(device)\n",
    "\n",
    "    # Use autocast for mixed precision\n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "\n",
    "    return probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJYP_qqj2p4z"
   },
   "outputs": [],
   "source": [
    "# Initialize the LIME text explainer\n",
    "explainer = LimeTextExplainer(class_names=['Democrat', 'Republican'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fldwRVpT4y4r"
   },
   "outputs": [],
   "source": [
    "# Function to explain predictions\n",
    "def explain_prediction(tweet):\n",
    "    explanation = explainer.explain_instance(tweet, predict_proba, num_features=10)\n",
    "    return explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O54e-jO5-nc7"
   },
   "outputs": [],
   "source": [
    "# # Example tweet to explain\n",
    "# sample_tweet = tweets[0]\n",
    "\n",
    "# # Explain the prediction\n",
    "# explanation = explain_prediction(sample_tweet)\n",
    "\n",
    "# # Show the explanation\n",
    "# explanation.show_in_notebook(text=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wWeQxRv7y66"
   },
   "outputs": [],
   "source": [
    "def predict_tweet(model, tweet, tokenizer, max_length, device):\n",
    "    # Tokenize and encode the tweet\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        tweet,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    # Move inputs to the appropriate device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    # Set model to evaluation mode and make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask)\n",
    "        _, prediction = torch.max(output.logits, dim=1)\n",
    "\n",
    "    # Map the prediction to the class label\n",
    "    class_label = 'Democrat' if prediction.item() == 0 else 'Republican'\n",
    "    return class_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFxasYxYBsQy"
   },
   "outputs": [],
   "source": [
    "def sort_tuples_array_by_second_item(tuples):\n",
    "    # Sort the tuples by the second item using the itemgetter function\n",
    "    return sorted(tuples, key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8adFH1J_Qsm"
   },
   "outputs": [],
   "source": [
    "def get_max_explained_words(txt):\n",
    "\n",
    "  prediction = predict_tweet(model, txt, tokenizer, MAX_LENGTH, device)\n",
    "  # print(\" \")\n",
    "  # print(\"prediction\")\n",
    "  # print(prediction)\n",
    "\n",
    "  exp = explain_prediction(txt)\n",
    "\n",
    "  exp_list = []\n",
    "  for x in zip(exp.local_exp[1], exp.as_list()):\n",
    "    exp_list.append((x[1][0], x[1][1], x[0][0]))\n",
    "\n",
    "  # print(\"exp_list\")\n",
    "  # print(exp_list)\n",
    "\n",
    "  # features with negative score are for Male class\n",
    "  democrat_list = list(filter(lambda x: x[1] < 0, exp_list))\n",
    "  democrat_list = sort_tuples_array_by_second_item(democrat_list)\n",
    "\n",
    "  # print(\"democrat_list\")\n",
    "  # print(democrat_list)\n",
    "  # print(len(democrat_list))\n",
    "\n",
    "  # features with positive score are for female class\n",
    "  republican_list = list(filter(lambda x: x[1] > 0, exp_list))\n",
    "  republican_list = sort_tuples_array_by_second_item(republican_list)\n",
    "\n",
    "  # print(\"republican_list\")\n",
    "  # print(republican_list)\n",
    "  # print(len(republican_list))\n",
    "\n",
    "  # min is used while the democrat score is negative\n",
    "  democrat_mc = min(democrat_list, key=itemgetter(1)) if len(democrat_list) else None\n",
    "\n",
    "  # print(\"democrat_mc\")\n",
    "  # print(democrat_mc)\n",
    "\n",
    "  # max is used while the republican score is negative\n",
    "  republican_mc = max(republican_list, key=itemgetter(1)) if len(republican_list) else None\n",
    "\n",
    "  # print(\"republican_mc\")\n",
    "  # print(republican_mc)\n",
    "\n",
    "  # if comment predicted Male\n",
    "  if prediction == \"Democrat\":\n",
    "    if len(democrat_list) > 1:\n",
    "      democrat_mc = democrat_list[0]\n",
    "      if (democrat_mc, 0) in words:\n",
    "        words[(democrat_mc[0], 0)]['lime_score'].extend(democrat_mc[1])\n",
    "        words[(democrat_mc[0], 0)]['position'] = democrat_mc[2]\n",
    "      else:\n",
    "        words[(democrat_mc[0], 0)] = {}\n",
    "        words[(democrat_mc[0], 0)]['lime_score'] = [democrat_mc[1]]\n",
    "        words[(democrat_mc[0], 0)]['position'] = democrat_mc[2]\n",
    "        wordsForCSV.append([democrat_mc[0], 0, democrat_mc[1]])\n",
    "\n",
    "      democrat_mc = democrat_list[1]\n",
    "      if (democrat_mc, 0) in words:\n",
    "        words[(democrat_mc[0], 0)]['lime_score'].extend(democrat_mc[1])\n",
    "        words[(democrat_mc[0], 0)]['position'] = democrat_mc[2]\n",
    "      else:\n",
    "        words[(democrat_mc[0], 0)] = {}\n",
    "        words[(democrat_mc[0], 0)]['lime_score'] = [democrat_mc[1]]\n",
    "        words[(democrat_mc[0], 0)]['position'] = democrat_mc[2]\n",
    "        wordsForCSV.append([democrat_mc[0], 0, democrat_mc[1]])\n",
    "    elif len(democrat_list) == 1:\n",
    "      democrat_mc = democrat_list[0]\n",
    "      if (democrat_mc, 0) in words:\n",
    "        words[(democrat_mc[0], 0)]['lime_score'].extend(democrat_mc[1])\n",
    "        words[(democrat_mc[0], 0)]['position'] = democrat_mc[2]\n",
    "      else:\n",
    "        words[(democrat_mc[0], 0)] = {}\n",
    "        words[(democrat_mc[0], 0)]['lime_score'] = [democrat_mc[1]]\n",
    "        words[(democrat_mc[0], 0)]['position'] = democrat_mc[2]\n",
    "        wordsForCSV.append([democrat_mc[0], 0, democrat_mc[1]])\n",
    "\n",
    "  else:\n",
    "    if len(republican_list) > 1:\n",
    "      republican_mc = republican_list[(len(republican_list)-1)]\n",
    "      if (republican_mc, 1) in words:\n",
    "        words[(republican_mc[0], 1)]['lime_score'].extend(republican_mc[1])\n",
    "        words[(republican_mc[0], 1)]['position'] = republican_mc[2]\n",
    "      else:\n",
    "        words[(republican_mc[0], 1)] = {}\n",
    "        words[(republican_mc[0], 1)]['lime_score'] = [republican_mc[1]]\n",
    "        words[(republican_mc[0], 1)]['position'] = republican_mc[2]\n",
    "        wordsForCSV.append([republican_mc[0], 1, republican_mc[1]])\n",
    "\n",
    "      republican_mc = republican_list[(len(republican_list)-2)]\n",
    "      if (republican_mc, 1) in words:\n",
    "        words[(republican_mc[0], 1)]['lime_score'].extend(republican_mc[1])\n",
    "        words[(republican_mc[0], 1)]['position'] = republican_mc[2]\n",
    "      else:\n",
    "        words[(republican_mc[0], 1)] = {}\n",
    "        words[(republican_mc[0], 1)]['lime_score'] = [republican_mc[1]]\n",
    "        words[(republican_mc[0], 1)]['position'] = republican_mc[2]\n",
    "        wordsForCSV.append([republican_mc[0], 1, republican_mc[1]])\n",
    "\n",
    "    elif len(republican_list) == 1:\n",
    "      republican_mc = republican_list[0]\n",
    "      if (republican_mc, 1) in words:\n",
    "        words[(republican_mc[0], 1)]['lime_score'].extend(republican_mc[1])\n",
    "        words[(republican_mc[0], 1)]['position'] = republican_mc[2]\n",
    "      else:\n",
    "        words[(republican_mc[0], 1)] = {}\n",
    "        words[(republican_mc[0], 1)]['lime_score'] = [republican_mc[1]]\n",
    "        words[(republican_mc[0], 1)]['position'] = republican_mc[2]\n",
    "        wordsForCSV.append([republican_mc[0], 1, republican_mc[1]])\n",
    "\n",
    "\n",
    "  return words, wordsForCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaTD8bc8F8Kh"
   },
   "outputs": [],
   "source": [
    "words = {}\n",
    "wordsForCSV = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khXsC0ye7zsC",
    "outputId": "b812ca0e-e242-4889-f2f4-b74ed188e860"
   },
   "outputs": [],
   "source": [
    "for tweet in tqdm(first_half_tweets, total = len(first_half_tweets)):\n",
    "  words, wordsForCSV = get_max_explained_words(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OS4WeQDcb7s"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header=[\"word\", \"label\", \"limescore\"]\n",
    "\n",
    "with open('/content/drive/MyDrive/Datasets/Tweets/24_extracted_strong_words_by_bert_base_uncased.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(wordsForCSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHpCGh8X7bKa"
   },
   "outputs": [],
   "source": [
    "# print(words)\n",
    "# print(wordsForCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgc5eFW8Furc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
