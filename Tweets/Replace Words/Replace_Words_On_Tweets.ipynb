{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DHQW9e5PhUEw",
    "outputId": "24ab0888-f87b-4d1b-a28b-232d9fd0a051"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6sIU-XeMO3U",
    "outputId": "3a940c55-8aae-4270-8bfe-93fc10b40457"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.en import English\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from scipy.special import softmax\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paGDVwJxMdXC",
    "outputId": "44431c76-d2d4-415f-a7b5-720b62198e4e"
   },
   "outputs": [],
   "source": [
    "glove_model = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sxbd19_br-c5"
   },
   "outputs": [],
   "source": [
    "symbols_dict = {\n",
    "    '!': 'Exclamation Mark',\n",
    "    '\"': 'Double Quotation Mark',\n",
    "    '#': 'Hash/Pound Sign',\n",
    "    '$': 'Dollar Sign',\n",
    "    '%': 'Percent Sign',\n",
    "    '&': 'Ampersand',\n",
    "    \"'\": 'Single Quotation Mark',\n",
    "    '(': 'Left Parenthesis',\n",
    "    ')': 'Right Parenthesis',\n",
    "    '*': 'Asterisk',\n",
    "    '+': 'Plus Sign',\n",
    "    ',': 'Comma',\n",
    "    '-': 'Hyphen',\n",
    "    '.': 'Period',\n",
    "    '/': 'Forward Slash',\n",
    "    ':': 'Colon',\n",
    "    ';': 'Semicolon',\n",
    "    '<': 'Less Than Sign',\n",
    "    '=': 'Equal Sign',\n",
    "    '>': 'Greater Than Sign',\n",
    "    '?': 'Question Mark',\n",
    "    '@': 'At Sign',\n",
    "    '[': 'Left Square Bracket',\n",
    "    '\\\\': 'Backslash',\n",
    "    ']': 'Right Square Bracket',\n",
    "    '^': 'Caret',\n",
    "    '_': 'Underscore',\n",
    "    '`': 'Backtick',\n",
    "    '{': 'Left Curly Brace',\n",
    "    '|': 'Vertical Bar',\n",
    "    '}': 'Right Curly Brace',\n",
    "    '~': 'Tilde'\n",
    "}\n",
    "\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours',\n",
    "              'yourself', 'yourselves', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "              'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have',\n",
    "              'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "              'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
    "              'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then',\n",
    "              'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "              'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\n",
    "              'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn',\n",
    "              \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
    "              'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "              \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'lol']\n",
    "\n",
    "stop_words_dict = {word: index for index, word in enumerate(stop_words)}\n",
    "\n",
    "sens_word_dict = {\n",
    "    'he': 'person',\n",
    "    'she': 'person',\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_d4y8gWMMjnS"
   },
   "outputs": [],
   "source": [
    "def get_vocab_and_doc(data_file_path):\n",
    "    df = pd.read_csv(data_file_path)\n",
    "\n",
    "    tweets = df['Tweet'].tolist()\n",
    "    labels = df['Party'].tolist()\n",
    "\n",
    "    n = int(len(tweets) / 15)\n",
    "    x = [tweets[i:i + n] for i in range(0, len(tweets), n)]\n",
    "    y = [labels[i:i + n] for i in range(0, len(labels), n)]\n",
    "\n",
    "    first_half_tweets = x[0]\n",
    "    second_half_tweets = x[1]\n",
    "    third_half_tweets = x[2]\n",
    "    fourth_half_tweets = x[3]\n",
    "    fifth_half_tweets = x[4]\n",
    "    sixth_half_tweets = x[5]\n",
    "    seventh_half_tweets = x[6]\n",
    "    eighth_half_tweets = x[7]\n",
    "    ninth_half_tweets = x[8]\n",
    "    tenth_half_tweets = x[9]\n",
    "    eleventh_half_tweets = x[10]\n",
    "    twelveth_half_tweets = x[11]\n",
    "    thirteenth_half_tweets = x[12]\n",
    "    fourteenth_half_tweets = x[13]\n",
    "    fifteen_half_tweets = x[14]\n",
    "\n",
    "    first_half_labels = y[0]\n",
    "    second_half_labels = y[1]\n",
    "    third_half_labels = y[2]\n",
    "    fourth_half_labels = y[3]\n",
    "    fifth_half_labels = x[4]\n",
    "    sixth_half_labels = x[5]\n",
    "    seventh_half_labels = x[6]\n",
    "    eighth_half_labels = x[7]\n",
    "    ninth_half_labels = x[8]\n",
    "    tenth_half_labels = x[9]\n",
    "    eleventh_half_labels = x[10]\n",
    "    twelveth_half_labels = x[11]\n",
    "    thirteenth_half_labels = x[12]\n",
    "    fourteenth_half_labels = x[13]\n",
    "    fifteen_half_labels = x[14]\n",
    "\n",
    "    vocab=Counter()\n",
    "\n",
    "    print(\"Building Vocabulary...\")\n",
    "\n",
    "    tokenizer = English()\n",
    "    docs = []\n",
    "    labels = []\n",
    "\n",
    "    for i in tqdm(range(len(first_half_tweets))):\n",
    "        tweet = first_half_tweets[i]\n",
    "        tokenized_text = [token.text for token in tokenizer(tweet)]\n",
    "        docs.append(tokenized_text)\n",
    "        labels.append(first_half_labels[i])\n",
    "        for token in tokenized_text:\n",
    "            vocab[token]+=1\n",
    "\n",
    "    return vocab, docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZHVUIJwcr9P"
   },
   "outputs": [],
   "source": [
    "# vocab, docs, labels = get_vocab_and_doc('/content/drive/MyDrive/Datasets/dataset.csv')\n",
    "# print(docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKKsmIAPMnP6"
   },
   "outputs": [],
   "source": [
    "def get_strong_words(data_file_path):\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    strong_words = df['word'].to_list()\n",
    "\n",
    "    word2id = {}\n",
    "    all_count = 1\n",
    "\n",
    "    for i in strong_words:\n",
    "        word2id[i] = all_count\n",
    "        all_count += 1\n",
    "\n",
    "    return word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yE5icVqrMpDs"
   },
   "outputs": [],
   "source": [
    "# def get_randomly_synonym_word(word):\n",
    "\n",
    "#     top_similar_words = 10\n",
    "\n",
    "#     try:\n",
    "#         word_vector = glove_model[word]\n",
    "#         similar_words = glove_model.most_similar(word_vector, topn=top_similar_words)\n",
    "\n",
    "#         words, score = zip(*similar_words)\n",
    "#         words = np.array(words)\n",
    "#         score = np.array(score)\n",
    "#         score = -score\n",
    "\n",
    "#         probabilities = softmax(1 * score / 2)\n",
    "\n",
    "#         # Randomly select an index based on the specified probabilities\n",
    "#         selected_index = np.random.choice(top_similar_words, p=probabilities)\n",
    "\n",
    "#         # return selected word\n",
    "#         return words[selected_index]\n",
    "\n",
    "#     except KeyError:\n",
    "#         return word\n",
    "\n",
    "def get_randomly_synonym_word(word):\n",
    "\n",
    "    top_similar_words = 10\n",
    "\n",
    "    try:\n",
    "        word_vector = glove_model[word]\n",
    "        similar_words = glove_model.most_similar(word_vector, topn=top_similar_words)\n",
    "\n",
    "        # Create a linearly decreasing probability distribution\n",
    "        # Adjust the slope and intercept as needed\n",
    "        slope = -0.2  # Slope of the linear decay\n",
    "        intercept = 1.2  # Intercept to ensure non-negative probabilities\n",
    "\n",
    "        # Calculate probabilities for each index\n",
    "        probabilities = [max(0, slope * i + intercept) for i in range(top_similar_words)]\n",
    "        probabilities = np.array(probabilities)\n",
    "\n",
    "        # Normalize probabilities to ensure they sum to 1\n",
    "        probabilities /= sum(probabilities)\n",
    "\n",
    "        # Randomly select an index based on the specified probabilities\n",
    "        selected_index = np.random.choice(top_similar_words, p=probabilities)\n",
    "\n",
    "        selected_synonym_word, selected_synonym_word_score = similar_words[selected_index]\n",
    "\n",
    "        return selected_synonym_word\n",
    "\n",
    "    except KeyError:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDxFPSMzMtse"
   },
   "outputs": [],
   "source": [
    "def santext():\n",
    "    vocab, docs, labels = get_vocab_and_doc('/content/drive/MyDrive/Datasets/Tweets/ExtractedTweets.csv')\n",
    "\n",
    "    strong_words = get_strong_words('/content/drive/MyDrive/Datasets/Tweets/BERT/combined_extracted_strong_words_without_duplicates.csv')\n",
    "\n",
    "    new_docs = []\n",
    "\n",
    "    p = 0.5 # get from arguments\n",
    "    for i in tqdm(range(len(docs))):\n",
    "        doc = docs[i]\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            if word.lower() in sens_word_dict:\n",
    "              new_doc.append(sens_word_dict[word.lower()])\n",
    "            elif word in strong_words and word.lower() not in symbols_dict and word.lower() not in stop_words_dict:\n",
    "                selected_synonym_word = word\n",
    "                temp = 0\n",
    "                flag = False\n",
    "                while True:\n",
    "                  selected_synonym_word = get_randomly_synonym_word(word.lower())\n",
    "\n",
    "                  if selected_synonym_word == word:\n",
    "                    break\n",
    "\n",
    "                  if selected_synonym_word not in strong_words:\n",
    "                    break\n",
    "\n",
    "                  temp = temp + 1\n",
    "                  if temp >= 10:\n",
    "                    flag = True\n",
    "                    break\n",
    "\n",
    "                if flag:\n",
    "                  new_doc.append(word)\n",
    "                else:\n",
    "                  new_doc.append(selected_synonym_word)\n",
    "            else:\n",
    "                flip_p=random.random()\n",
    "                # if word.lower() in sens_word_dict:\n",
    "                #   new_doc.append(sens_word_dict[word.lower()])\n",
    "                if flip_p<=p and word.lower() not in symbols_dict and word.lower() not in stop_words_dict:\n",
    "                    selected_synonym_word = get_randomly_synonym_word(word.lower())\n",
    "                    new_doc.append(selected_synonym_word)\n",
    "                else:\n",
    "                    new_doc.append(word)\n",
    "\n",
    "        new_doc = \" \".join(new_doc)\n",
    "        new_docs.append(new_doc)\n",
    "\n",
    "    df = pd.DataFrame(list(zip(labels,new_docs)), columns=['party', 'tweet'])\n",
    "    df.to_csv('/content/drive/MyDrive/Datasets/Tweets/BERT/1_tweet_replaced_dataset_top10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBWFEDDONndb",
    "outputId": "1876f7e3-1715-421f-cbf5-1b58ee0cbbc9"
   },
   "outputs": [],
   "source": [
    "santext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUB8ZK3cbz2S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
