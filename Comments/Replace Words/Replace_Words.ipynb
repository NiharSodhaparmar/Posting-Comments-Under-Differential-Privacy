{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6sIU-XeMO3U",
        "outputId": "9e4164e7-d958-4417-8a99-10b7ac88f11c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from spacy.lang.en import English\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paGDVwJxMdXC",
        "outputId": "c2b8a630-b93e-45ee-817e-fb296162a355"
      },
      "outputs": [],
      "source": [
        "glove_model = api.load(\"glove-wiki-gigaword-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxbd19_br-c5"
      },
      "outputs": [],
      "source": [
        "symbols_dict = {\n",
        "    '!': 'Exclamation Mark',\n",
        "    '\"': 'Double Quotation Mark',\n",
        "    '#': 'Hash/Pound Sign',\n",
        "    '$': 'Dollar Sign',\n",
        "    '%': 'Percent Sign',\n",
        "    '&': 'Ampersand',\n",
        "    \"'\": 'Single Quotation Mark',\n",
        "    '(': 'Left Parenthesis',\n",
        "    ')': 'Right Parenthesis',\n",
        "    '*': 'Asterisk',\n",
        "    '+': 'Plus Sign',\n",
        "    ',': 'Comma',\n",
        "    '-': 'Hyphen',\n",
        "    '.': 'Period',\n",
        "    '/': 'Forward Slash',\n",
        "    ':': 'Colon',\n",
        "    ';': 'Semicolon',\n",
        "    '<': 'Less Than Sign',\n",
        "    '=': 'Equal Sign',\n",
        "    '>': 'Greater Than Sign',\n",
        "    '?': 'Question Mark',\n",
        "    '@': 'At Sign',\n",
        "    '[': 'Left Square Bracket',\n",
        "    '\\\\': 'Backslash',\n",
        "    ']': 'Right Square Bracket',\n",
        "    '^': 'Caret',\n",
        "    '_': 'Underscore',\n",
        "    '`': 'Backtick',\n",
        "    '{': 'Left Curly Brace',\n",
        "    '|': 'Vertical Bar',\n",
        "    '}': 'Right Curly Brace',\n",
        "    '~': 'Tilde'\n",
        "}\n",
        "\n",
        "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours',\n",
        "              'yourself', 'yourselves', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
        "              'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have',\n",
        "              'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
        "              'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
        "              'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then',\n",
        "              'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
        "              'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\n",
        "              'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn',\n",
        "              \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
        "              'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
        "              \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'lol']\n",
        "\n",
        "stop_words_dict = {word: index for index, word in enumerate(stop_words)}\n",
        "\n",
        "sens_word_dict = {\n",
        "    'he': 'person',\n",
        "    'she': 'person',\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d4y8gWMMjnS"
      },
      "outputs": [],
      "source": [
        "def get_vocab_and_doc(data_file_path):\n",
        "    df = pd.read_csv(data_file_path)\n",
        "\n",
        "    comments = df['comment'].tolist()\n",
        "    genders = df['user_gender'].tolist()\n",
        "\n",
        "    vocab=Counter()\n",
        "\n",
        "    print(\"Building Vocabulary...\")\n",
        "\n",
        "    tokenizer = English()\n",
        "    docs = []\n",
        "    labels = []\n",
        "\n",
        "    for i in tqdm(range(len(comments))):\n",
        "        comment = comments[i]\n",
        "        tokenized_text = [token.text for token in tokenizer(comment)]\n",
        "        docs.append(tokenized_text)\n",
        "        labels.append(genders[i])\n",
        "        for token in tokenized_text:\n",
        "            vocab[token]+=1\n",
        "\n",
        "    return vocab, docs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZHVUIJwcr9P"
      },
      "outputs": [],
      "source": [
        "# vocab, docs, labels = get_vocab_and_doc('/content/drive/MyDrive/Datasets/dataset.csv')\n",
        "# print(docs[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKKsmIAPMnP6"
      },
      "outputs": [],
      "source": [
        "def get_strong_words(data_file_path):\n",
        "    df = pd.read_csv(data_file_path)\n",
        "    strong_words = df['word'].to_list()\n",
        "\n",
        "    word2id = {}\n",
        "    all_count = 1\n",
        "\n",
        "    for i in strong_words:\n",
        "        word2id[i] = all_count\n",
        "        all_count += 1\n",
        "\n",
        "    return word2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE5icVqrMpDs"
      },
      "outputs": [],
      "source": [
        "def get_randomly_synonym_word(word):\n",
        "\n",
        "    top_similar_words = 10\n",
        "\n",
        "    try:\n",
        "        word_vector = glove_model[word]\n",
        "        similar_words = glove_model.most_similar(word_vector, topn=top_similar_words)\n",
        "\n",
        "        # Create a linearly decreasing probability distribution\n",
        "        # Adjust the slope and intercept as needed\n",
        "        slope = -0.2  # Slope of the linear decay\n",
        "        intercept = 1.2  # Intercept to ensure non-negative probabilities\n",
        "\n",
        "        # Calculate probabilities for each index\n",
        "        probabilities = [max(0, slope * i + intercept) for i in range(top_similar_words)]\n",
        "        probabilities = np.array(probabilities)\n",
        "\n",
        "        # Normalize probabilities to ensure they sum to 1\n",
        "        probabilities /= sum(probabilities)\n",
        "\n",
        "        # Randomly select an index based on the specified probabilities\n",
        "        selected_index = np.random.choice(top_similar_words, p=probabilities)\n",
        "\n",
        "        selected_synonym_word, selected_synonym_word_score = similar_words[selected_index]\n",
        "\n",
        "        return selected_synonym_word\n",
        "\n",
        "    except KeyError:\n",
        "        return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDxFPSMzMtse"
      },
      "outputs": [],
      "source": [
        "def santext():\n",
        "    vocab, docs, labels = get_vocab_and_doc('/content/drive/MyDrive/Datasets/dataset.csv')\n",
        "\n",
        "    strong_words = get_strong_words('/content/drive/MyDrive/Datasets/Bert Modal On One Comment Full Data/combined_extracted_strong_words_without_duplicates_one_comment.csv')\n",
        "\n",
        "    new_docs = []\n",
        "\n",
        "    p = 0.5 # get from arguments\n",
        "    for i in tqdm(range(len(docs))):\n",
        "        doc = docs[i]\n",
        "        new_doc = []\n",
        "        for word in doc:\n",
        "            if word in sens_word_dict:\n",
        "              new_doc.append(sens_word_dict[word])\n",
        "            elif word in strong_words and word.lower() not in symbols_dict and word.lower() not in stop_words_dict:\n",
        "                selected_synonym_word = word\n",
        "                temp = 0\n",
        "                flag = False\n",
        "                while True:\n",
        "                  selected_synonym_word = get_randomly_synonym_word(word.lower())\n",
        "\n",
        "                  if selected_synonym_word == word:\n",
        "                    break\n",
        "\n",
        "                  if selected_synonym_word not in strong_words:\n",
        "                    break\n",
        "\n",
        "                  temp = temp + 1\n",
        "                  if temp >= 10:\n",
        "                    flag = True\n",
        "                    break\n",
        "\n",
        "                if flag:\n",
        "                  new_doc.append(word)\n",
        "                else:\n",
        "                  new_doc.append(selected_synonym_word)\n",
        "            else:\n",
        "                flip_p=random.random()\n",
        "                if word in sens_word_dict:\n",
        "                  new_doc.append(sens_word_dict[word])\n",
        "                elif flip_p<=p and word.lower() not in symbols_dict and word.lower() not in stop_words_dict:\n",
        "                    selected_synonym_word = get_randomly_synonym_word(word.lower())\n",
        "                    new_doc.append(selected_synonym_word)\n",
        "                else:\n",
        "                    new_doc.append(word)\n",
        "\n",
        "        new_doc = \" \".join(new_doc)\n",
        "        new_docs.append(new_doc)\n",
        "\n",
        "    df = pd.DataFrame(list(zip(labels,new_docs)), columns=['user_gender', 'comment'])\n",
        "    df.to_csv('/content/drive/MyDrive/Datasets/Bert Modal On One Comment Full Data/replaced_dataset_one_comment.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBWFEDDONndb",
        "outputId": "86d0997e-43ae-430c-ab86-fde464ac4e32"
      },
      "outputs": [],
      "source": [
        "santext()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji6r0RchqLfJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YTsfPAX39PX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
